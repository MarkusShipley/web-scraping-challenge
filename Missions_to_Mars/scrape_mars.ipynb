{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# Import Dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import html5lib\n",
    "import lxml\n",
    "from splinter import Browser\n",
    "import pymongo\n",
    "from flask import Flask, render_template, redirect\n",
    "from flask_pymongo import PyMongo\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "        # In[18]:\n",
    "\n",
    "\n",
    "        # Set the executable path and initialize Splinter\n",
    "\n",
    "executable_path = {\"executable_path\": \"./chromedriver.exe\"}\n",
    "browser = Browser(\"chrome\", **executable_path)\n",
    "\n",
    "def scrape():\n",
    "        # In[13]:\n",
    "\n",
    "        mars_info_dictionary = dict()\n",
    "        # NASA Mars News \n",
    "        # Scrape the [Mars News Site](https://redplanetscience.com/) and collect the latest News Title and Paragraph Text. \n",
    "        # Assign the text to variables that you can reference later.\n",
    "\n",
    "        # Visit the Maars News Site provided in the insructions\n",
    "\n",
    "        url = 'https://redplanetscience.com/'\n",
    "        browser.visit(url)\n",
    "\n",
    "\n",
    "        # In[15]:\n",
    "\n",
    "\n",
    "        # News Title\n",
    "\n",
    "        # Using Beautiful Soup, the broswer html will be converted to a soup object.Convert the browser html to a soup object.\n",
    "        # To collect the latest News Title and Paragraph Text. I am going to the site and inspect the onject to get the classes for\n",
    "        # the title and paragraph. According to inspect process <div class=\"content_title> is the class\n",
    "        # Note: Can use soup because that is the name assigned when importing in bs.\n",
    "\n",
    "        html = browser.html\n",
    "        soup = bs(html, 'html.parser')\n",
    "        news_title = soup.find('div', class_='content_title')\n",
    "        news_title.text\n",
    "\n",
    "\n",
    "        # In[16]:\n",
    "\n",
    "\n",
    "        # After locating the headline, referenced the news_title variable and the .text.strip() functions to make the news title\n",
    "        # regular text without the html markup\n",
    "        #https://python.hotexamples.com/examples/bs4/BeautifulSoup/strip/python-beautifulsoup-strip-method-examples.html\n",
    "\n",
    "        news_title = news_title.text.strip()\n",
    "        #print(news_title)\n",
    "        mars_info_dictionary['news_titles'] = news_title\n",
    "\n",
    "        # In[17]:\n",
    "\n",
    "\n",
    "        #News Paragraph \n",
    "\n",
    "        # According to the inspect process <div class=\"article_teaser_body\">\n",
    "\n",
    "        news_paragraph = soup.find('div', class_='article_teaser_body')\n",
    "        news_paragraph\n",
    "\n",
    "        # In[18]:\n",
    "\n",
    "\n",
    "        # After locating the paragraph, referenced the news_paragraph variable and the .text.strip() functions to make the \n",
    "        # paragraph regular text without the html markup\n",
    "        #https://python.hotexamples.com/examples/bs4/BeautifulSoup/strip/python-beautifulsoup-strip-method-examples.html\n",
    "\n",
    "        news_paragraph = news_paragraph.text.strip()\n",
    "        #print(news_paragraph)\n",
    "        mars_info_dictionary['news_paragraph'] = news_paragraphs\n",
    "              \n",
    "        # In[8]:\n",
    "\n",
    "\n",
    "        # JPL Mars Space Images - Featured Image\n",
    "\n",
    "\n",
    "        # In[27]:\n",
    "\n",
    "\n",
    "        # Visit the url for the Featured Space Image site [here](https://spaceimages-mars.com).\n",
    "        # Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the \n",
    "        # url string to a variable called `featured_image_url`.\n",
    "        # Make sure to find the image url to the full size `.jpg` image.\n",
    "        # Make sure to save a complete url string for this image.\n",
    "\n",
    "        url_2 = \"https://spaceimages-mars.com/\"\n",
    "        browser.visit(url_2)\n",
    "\n",
    "\n",
    "        html = browser.html\n",
    "        soup = bs(html, 'html.parser')\n",
    "        img = soup.find('img', class_='headerimage fade-in')\n",
    "        img_link = img['src']\n",
    "\n",
    "        featured_image_url = url_2 + img_link\n",
    "        \n",
    "        mars_info_dictionary ['featured_image'] = featured_image_url \n",
    "\n",
    "        # In[11]:\n",
    "\n",
    "\n",
    "        ###Scrape the table containing facts about diameter of Mars\n",
    "        #!pip install html5lib\n",
    "\n",
    "        url_3 = \"https://galaxyfacts-mars.com/\"\n",
    "\n",
    "\n",
    "        # In[13]:\n",
    "\n",
    "        df = pd.read_html(url_3)\n",
    "        display(df[1])\n",
    "        mars_df = (df[1])\n",
    "        mars_df\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "        # Converting df to html in case it is needed for the flask portion of the exercise\n",
    "        mars_df.to_html(buf=None, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None,index_names=True, justify=None, bold_rows=True, classes=None, escape=True, max_rows=None, max_cols=None, show_dimensions=False, notebook=False)\n",
    "\n",
    "        mars_info_dictionary['mars_facts'] = mars_df\n",
    "\n",
    "        # In[50]:\n",
    "\n",
    "\n",
    "        #Create empty dictionary for results\n",
    "        hemisphere_image_urls = {}\n",
    "\n",
    "        #Define the url\n",
    "        url_4 = \"https://marshemispheres.com/\"\n",
    "\n",
    "        #Go to main website\n",
    "        browser.visit(url_4)\n",
    "\n",
    "        #Read page as HTML\n",
    "        html = browser.html\n",
    "\n",
    "        #Parse HTML\n",
    "        soup = bs(html, 'html.parser')\n",
    "\n",
    "        #Find area that represents the list with hemispheres\n",
    "        nav_list = soup.find('div', class_=\"collapsible results\")\n",
    "\n",
    "        #From the above area find every item in the list\n",
    "        all_hemispheres = nav_list.find_all('div', class_=\"item\")\n",
    "\n",
    "        #Extract image link for each hemisphere\n",
    "        for hemisphere in all_hemispheres:\n",
    "            #Find hemisphere name\n",
    "            hemisphere_name = hemisphere.find('h3').text\n",
    "\n",
    "            #Find the link to hemiphere page\n",
    "            link = hemisphere.find('a', class_ = \"itemLink product-item\")['href']\n",
    "\n",
    "            #Create a full URL for hemishphere page\n",
    "            hemisphere_link = url_4 + link\n",
    "\n",
    "            #Go to hemisphere page\n",
    "            browser.visit(hemisphere_link)\n",
    "\n",
    "            #Read page as HTML\n",
    "            html = browser.html\n",
    "\n",
    "            #Parse HTML\n",
    "            soup = bs(html, 'html.parser')\n",
    "\n",
    "            #Extract link to full image\n",
    "            img = soup.find('img', class_='wide-image')\n",
    "            hemisphere_img_link = img['src']\n",
    "\n",
    "            #Create a full URL for hemisphere image\n",
    "            hemisphere_image_url = url_4 + hemisphere_img_link\n",
    "\n",
    "            #Add hemisphere name and hemisphere url to dictionary with results\n",
    "            hemisphere_image_urls[hemisphere_name] = hemisphere_image_url\n",
    "        \n",
    "        # In[51]:\n",
    "        hemisphere_image_urls\n",
    "\n",
    "        mars_info_dictionary['hemisphere_images'] = hemisphere_image_urls\n",
    "        \n",
    "        mars = {\n",
    "            \"News_Title\": mars_info_dictionary[\"news_titles\"],\n",
    "            \"News_Summary\": mars_info_dictionary[\"news_paragraph\"],\n",
    "            \"Featured_Image\": mars_info_dictionary[\"featured_image\"],\n",
    "            \"Facts_Table\": mars_info_dictionary[\"mars_fact\"],\n",
    "            \"Hemisphere_Image_urls\": mars_info_dictionary['hemisphere_images']\n",
    "        }\n",
    "\n",
    "        return mars\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-PythonData] *",
   "language": "python",
   "name": "conda-env-.conda-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
